{"meta":{"title":"非洲卖红薯的","subtitle":null,"description":null,"author":"非洲卖红薯的","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"hadoop常用命令","slug":"hadoop常用命令","date":"2018-12-15T13:51:08.000Z","updated":"2018-12-15T13:53:15.445Z","comments":true,"path":"2018/12/15/hadoop常用命令/","link":"","permalink":"http://yoursite.com/2018/12/15/hadoop常用命令/","excerpt":"FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。","text":"FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。 cat使用方法：hadoop fs -cat URI [URI …]将路径指定文件的内容输出到stdout。示例： hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2 hadoop fs -cat file:///file3 /user/hadoop/file4返回值：成功返回0，失败返回-1。chgrp使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. –&gt;改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。chmod使用方法：hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …]改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。chown使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。copyFromLocal使用方法：hadoop fs -copyFromLocal URI除了限定源路径是一个本地文件外，和put命令相似。copyToLocal使用方法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI 除了限定目标路径是一个本地文件外，和get命令类似。cp使用方法：hadoop fs -cp URI [URI …] 将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。示例： hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir返回值：成功返回0，失败返回-1。du使用方法：hadoop fs -du URI [URI …]显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。示例：hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1返回值：成功返回0，失败返回-1。 dus使用方法：hadoop fs -dus 显示文件的大小。expunge使用方法：hadoop fs -expunge清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。get使用方法：hadoop fs -get [-ignorecrc] [-crc] 复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。示例： hadoop fs -get /user/hadoop/file localfile hadoop fs -get hdfs://host:port/user/hadoop/file localfile返回值：成功返回0，失败返回-1。getmerge使用方法：hadoop fs -getmerge [addnl]接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。ls使用方法：hadoop fs -ls 如果是文件，则按照如下格式返回文件信息：文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：目录名 修改日期 修改时间 权限 用户ID 组ID示例：hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile返回值：成功返回0，失败返回-1。 lsr使用方法：hadoop fs -lsr ls命令的递归版本。类似于Unix中的ls -R。 mkdir使用方法：hadoop fs -mkdir 接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。示例： hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir返回值：成功返回0，失败返回-1。movefromLocal使用方法：dfs -moveFromLocal 输出一个”not implemented“信息。mv使用方法：hadoop fs -mv URI [URI …] 将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。示例： hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1返回值：成功返回0，失败返回-1。put使用方法：hadoop fs -put … 从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。 hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile hadoop fs -put - hdfs://host:port/hadoop/hadoopfile从标准输入中读取输入。返回值：成功返回0，失败返回-1。rm使用方法：hadoop fs -rm URI [URI …]删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。示例： hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir返回值：成功返回0，失败返回-1。rmr使用方法：hadoop fs -rmr URI [URI …]delete的递归版本。示例： hadoop fs -rmr /user/hadoop/dir hadoop fs -rmr hdfs://host:port/user/hadoop/dir返回值：成功返回0，失败返回-1。setrep使用方法：hadoop fs -setrep [-R] 改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。示例： hadoop fs -setrep -w 3 -R /user/hadoop/dir1返回值：成功返回0，失败返回-1。stat使用方法：hadoop fs -stat URI [URI …]返回指定路径的统计信息。示例： hadoop fs -stat path返回值：成功返回0，失败返回-1。tail使用方法：hadoop fs -tail [-f] URI将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。示例： hadoop fs -tail pathname返回值：成功返回0，失败返回-1。test使用方法：hadoop fs -test -[ezd] URI选项：-e 检查文件是否存在。如果存在则返回0。-z 检查文件是否是0字节。如果是则返回0。-d 如果路径是个目录，则返回1，否则返回0。示例： hadoop fs -test -e filenametext使用方法：hadoop fs -text 将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。touchz使用方法：hadoop fs -touchz URI [URI …]创建一个0字节的空文件。示例： hadoop fs -touchz pathname返回值：成功返回0，失败返回-1。","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"}]},{"title":"灵活的DBContext生命周期管理","slug":"灵活的DBContext生命周期管理","date":"2018-12-15T10:15:52.000Z","updated":"2018-12-15T14:30:58.886Z","comments":true,"path":"2018/12/15/灵活的DBContext生命周期管理/","link":"","permalink":"http://yoursite.com/2018/12/15/灵活的DBContext生命周期管理/","excerpt":"我们在开发过程中总是回遇到的许多架构上的问题，在这些架构问题中，最常见的是 DbContext 生命周期的管理以及跨域调用时遇到的 request 上下文限制的问题。","text":"我们在开发过程中总是回遇到的许多架构上的问题，在这些架构问题中，最常见的是 DbContext 生命周期的管理以及跨域调用时遇到的 request 上下文限制的问题。&nbsp;首先说说 DBContext的Scope 定义问题。根据微软的建议， DBContext是一个轻量级的，类似于 Unit of work模式的实体上下文，其生命周期对应一次业务操作，比如一次 REST API 调用，一次线程中的批处理业务等等，但总的原则是其生命周期应该尽可能短，另外 DBContext的生命周期也隐含地对应一次数据库事务。&nbsp;DBContext生命周期定义有许多的方式，如在一个 DAO函数中创建 DBContext，并在离开函数前销毁的模式，或者在HTTP request开头创建，在 HTTP request结尾销毁的模式，这些模式各有利弊，在一个 DAO函数中创建 DBContext这中模式较好地避免了死锁的发生，但是频繁且碎片化的生命周期导致事务完全失效且对性能有一定的影响，HTTP request的方式灵活性较差，一旦脱离 web上下文，就难以找到更合适的粒度，并且完全依赖运行环境上下文的模式为业务的在不同环境下的复用带来了较大的限制。 这个DBContext的模式有一个共同的问题，就是 DBContext生命周期的定义是不可变的，不能根据业务的需要进行覆盖和重新定义 ，这也是导致我们目前开发中遇到各种问题和障碍的主要原因。&nbsp;DBContext以及其对应的生命周期归根到底是一种资源（类似于 Session），既然是资源，就应该被灵活使用， 在不同的运行环境中拥有不同的行为，而在我们开发 Service和业务逻辑时，并不能完全确定资源的使用方式，我们可以定义 DBContext在当前业务下的生命周期，但是当局部的业务逻辑被放到更大或者完全不同的业务层面中时，作为资源（ Session）是需要被重新定义以适应新的业务上下文的。所以，我们需要一种可以在任何粒度下使用，并可以按需覆盖的 DBContext生命周期管理架构，以便我们在任何业务上下文中都能灵活控制事务和生命周期。&nbsp;针对这个需求，目前已经有不少成熟的思路和解决方案，比较好的是DbContextScope模式和其对应的开源代码，可以参考其在 GitHub上的工程：&nbsp;https://github.com/mehdime/DbContextScope&nbsp;DbContextScope使用.Net 的CallContext机制（ http://www.cnblogs.com/vwxyzh/archive/2009/02/21/1395416.html）来实现同一调用上下文中资源的共享，其典型的调用方式如下面的截图所示：使用中在 Service层中每个可能被单独调用的方法中显示声明 DBContextScope，当同一AppDomain 中如果有嵌套调用，最外部的 scope会保证所有内部的scope都使用同一个 DBContext，保证了事务和生命周期能被灵活的管理，大家还可以根据不同的业务来选择最合适的 scope，但是即使大家没有精力考虑生命周期的合理定义，也只需要简单的在所有函数中都加上 DBContextScope，就能保证适应绝大多数的调用场景。使用这种模式后，大家也不再需要依赖 HTTP Request或者将Service 声明为多实例，为开发提供更多的灵活性。&amp;另外针对跨域调用的限制， CallContext也是解决该问题的合理方案， Plugin Framework随后会提供一套统一的框架共大家在跨域调用时将必要的资源上下文通过 CallContext跨域传递给被调用方，包括在 Web Controller中对跨同一AppDomain 的service多次调用的 DBContext的生命周期进行统一管理，以及提供方法让大家将 HTTP Context中的必要信息传递给跨域的 Service层。","categories":[],"tags":[{"name":"数据库、.Net","slug":"数据库、-Net","permalink":"http://yoursite.com/tags/数据库、-Net/"}]},{"title":"Redis常用命令","slug":"Redis常用命令","date":"2018-12-15T10:15:52.000Z","updated":"2018-12-15T10:21:31.484Z","comments":true,"path":"2018/12/15/Redis常用命令/","link":"","permalink":"http://yoursite.com/2018/12/15/Redis常用命令/","excerpt":"","text":"# 命令 参数 例子 功能 返回值 原理 1 keys pattern: 键名配置模式 keys * 输出所有键 返回键列表 遍历所有键，和输入的模式做匹配，时间复杂度为O(n)，当Redis保存大量键时，线上环境禁止使用 2 dbsize dbsize 获取键总数 返回键总数 直接获取Redis内置键总数变量，时间复杂度为O(1) 3 exists key: 键名 exists key 检查键是否存在 1：键存在 0：键不存在 4 del key: 键名列表 del key1 key2 删除键 1：删除成功 0：键不存在 5 expire key: 键名 seconds:过期时间 expire key1 10 键过期 1：设置成功 0：键不存在 6 ttl key:键名 ttl key1 获取剩余过期时间 &gt;=0：键剩余的过期时间 -1：键没有设置过期时间 -2：键不存在 7 type key: 键名 type key1 获取键的数据类型 返回键类型 如果键不存在则返回none 8 set key: 键名 value: 值 set name hello 设置键 9 get key:键名 get name 获取键值 键值","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"JavaScript作用域","slug":"JavaScript作用域","date":"2018-12-15T08:56:15.000Z","updated":"2018-12-15T09:54:56.106Z","comments":true,"path":"2018/12/15/JavaScript作用域/","link":"","permalink":"http://yoursite.com/2018/12/15/JavaScript作用域/","excerpt":"JaveScript 是Web前端开发的最重要的语言，在JavaScript中作用域是一个重要的基础知识点。在JavaScript中共有三种作用域，分别是全局作用域、函数左右域和块级作用域。本文会先对三种作用域做介绍，之后举一个常见的例子做整体介绍。","text":"JaveScript 是Web前端开发的最重要的语言，在JavaScript中作用域是一个重要的基础知识点。在JavaScript中共有三种作用域，分别是全局作用域、函数左右域和块级作用域。本文会先对三种作用域做介绍，之后举一个常见的例子做整体介绍。 1、全局作用域全局作用域，故名思议，全局作用域就是整个JavaScript代码范围。如下代码所示: var a = &quot;Hello&quot;;console.log(a); //Hellofor(var i in a){ console.log(a[i]); //H、e、l、l、o} console.log(i); // 4}function func(){ console.log(a); //Hello console.log(i) // 4}func();在这段代码中，我们先声明了变量a，并对a赋值Hello，a是我们在全局上声明的变量，此时a的作用域为全局作用域，我们之后在直接输出、循环中输出、函数中输出都没有错误，说明此时a在任何位置均可以访问。同时在for循环中我们声明了变量i，for循环结束之后变量i并没有释放，而是输出了最后的值4，说明此时变量i的作用域为全局作用域。 2、函数作用域函数作用域，故名思议，函数作用域就是生个函数范围。如下代码所示: function fun() { var a = &quot;Hello&quot;; var i = 0; switch (i) { case 0: console.log(a); //hello } console.log(a); //hello}func();console.log(a); //Errorvar t = &quot;hello2&quot;;function fun2() { var t = &quot;helllo3&quot;; console.log(t); //hello3}在这段代码中，我们先声明了函数fun，在函数内部我们声明了变量a，此时a的作用域为函数作用域，之后我们在switch中输出，尽管我们的switch被大括号包裹，但是我们依然可以访问，说明函数作用域可以在函数的任意位置范围，之后在函数之外，我们输出变量a，此时就会出错，因为变量a的作用域为函数作用域，在函数为无法访问。在fun2中，我们声明了一个与全局变量同名的变量t，此时函数作用域中的t为覆盖全局作用域的t，这里的规则是子作用域覆盖父作用域，而全局作用域做为所有作用域的顶层作用域，一定会被子作用域的同名变量覆盖。 3、块级作用域在ES6中，JavaScript引入了块级作用域，块级作用域的作用域范围为代码块。如下代码所示： &quot;use strict&quot;{ var a = 0; console.log(a);}console.log(a) //Error在上面代码中，我们使用use strict启用ES6，此时块级作用域起效，在之后的代码块中，变量a为块级作用域，在代码中访问正常，而在代码我们无法访问，与函数作用域类似，我们依然可以访问父作用域的变量，当发生变量重名是会覆盖父作用域的变量。 4、例子与作用域相关的问题中，大多出在for中，如下代码所示： var funcs = [];for(var i = 0;i&lt;4;i++)&nbsp;{ funcs.push(function()&nbsp;{ console.log(i); });}funcs.forEach(function()&nbsp;{ fun();});我们执行这段代码，此时我们期望输出0、1、2、3，但是在实际运行时，我们输出了4、4、4、4，这是因为var声明的变量只能是全局作用域或函数作用域，如果我们这段代码运行在全局作用域的，那么变量i的作用域就是全局作用域，在整个作用域中变量i只有一个值，也就是在funs中我们引用的变量i和全局作用域的i是同一个并且只有一个，当循环结束后，变量的值变成了4，我们在执行funs，那么输出的就是变量i的值4。那么很明显这个问题是作用域没有独立引起的，解决办法就是新建一样作用域。 4.1 方法1var funcs = [];for (var i = 0; i &lt; 4; i++) { funcs.push((function (in_i) { return function () { console.log(in_i); } })(i));}funcs.forEach(function () { fun();});如上面的代码所示，我们使用闭包，闭包就是一个立即执行的函数，此时我们使用闭包新建了一个函数作用域，并把变量i参入，在闭包内部会拷贝一份变量i到in_i，此时for内部不再是单一作用域，做个作用域有多个in_i。 4.2 方法2var funcs = [];for(let i = 0;i&lt;4;i++) { funcs.push(function() { console.log(i); });}funcs.forEach(function() { fun();});如上代码所示，我们将var替换成了let，let是ES6声明变量的方法，此时let声明后i变成了块级作用域变量，也就是新建了一个块级作用域，在for内部i不再是同一个，而是在一个作用域中一个变量i。","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"}]}]}